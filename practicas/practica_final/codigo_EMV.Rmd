---
title: "Práctica Final"
author: "Alejandro Cárdenas Barranco, Álvaro Rodríguez Gallardo, Juan Manuel Rodríguez Gómez"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: 6
    number_sections: yes
    toc_float:
      collapsed: yes
      smooth_scroll: no
  word_document:
    toc: yes
    toc_depth: '6'
  pdf_document: default
---
<style>
.math {
  font-size: 8.25pt;options(encoding = 'UTF-8')
}
</style>

<div style="text-align: justify">

En esta práctica se va a analizar un dataset que aborda el rendimiento en la asignatura Matemáticas de los alumnos en educación secundaria de dos centros escolares portugueses. Los datos se han recopilado mediante el uso de informes y cuestionarios escolares. Dicho dataset se puede descargar mediante el siguiente enlace:

https://archive.ics.uci.edu/dataset/320/student+performance

En primer lugar, se realizará un **análisis exploratorio** previo de los datos para identificar posibles **valores perdidos** y **valores extremos o “outliers”**, y se tomarán las decisiones correspondientes para tratarlos.

En segundo lugar, se realizará un **Análisis de Componentes Principales (ACP)**, esto es, se va a condensar la información aportada por las variables originales en unas pocas combinaciones lineales de ellas, con el objetivo de hacer una reducción de la dimensión. Estas se buscan con máxima varianza y perpendiculares entre sí, de forma que cada una sigue la dirección en la que las observaciones varían más y no está correlacionada con las anteriores.

En tercer lugar, se realizará un **Análisis Factorial (AF)**, esto es, se van a identificar variables latentes (no observables) con una alta correlación con un grupo de variables observables y correlación prácticamente nula con el resto. Así, se hará una reducción de la dimensión.

En cuarto lugar, se realizará un **Análisis Discriminante Lineal (ADL)** y otro **Cuadrático (ADC)**, verificando las hipótesis necesarias de normalidad. El Análisis Discriminante es un método de clasificación de variables cualitativas que permitirá clasificar nuevas observaciones según sus características (variables explicativas o predictores) en las distintas categorías de la variable cualitativa respuesta.

Finalmente, se realizará un **Análisis Cluster (AC)**. El Análisis Cluster es una técnica multivariante cuyo principal objetivo es agrupar objetos formando conglomerados (clusters) con un alto grado de homogeneidad interna y heterogeneidad externa. En otras palabras, el AC es un procedimiento exploratorio que permite encontrar estructuras con similitudes en un conjunto de datos con cierta variabilidad.

# **Cargando Paquetes y Leyendo el Conjunto de Datos**

## Cargar e instalar paquetes de R

El siguiente módulo de código fuente se encarga de cargar, si ya están instalados, todos los paquetes que se utilizarán en esta sesión de R. Si bien un paquete R se puede cargar en cualquier momento cuando se vaya a utilizar, es recomendable optimizar sus llamadas con este fragmento de código al principio.

Cargar un paquete en una sesión de R **requiere que ya esté instalado**. Si no es así, el primer paso es ejecutar la sentencia:

*install.packages("name_of_the_library")*


```{r warning=FALSE, message=FALSE}

#########################################
# Loading necessary packages and reason #
#########################################

# This is an example of the first installation of a package
# Only runs once if the package is not installed
# Once it is installed this sentence has to be commented (not to run again)
# install.packages("summarytools")

# Package required to call 'read_xlsx' function (loading '.xlsx' data format)
library(readxl)

# Package required to call 'freq' and 'descr' functions (descriptive statistics)
library(summarytools)

# Package required to call 'factanal' function
library(stats)

# Package required to call 'stat.desc' function
library(pastecs)

# Package required to call 'cortest.bartlett' function
library(psych)

# Package required to call 'hetcor' function
library(polycor)

# Package required to call 'ggcorrplot' function
library(ggcorrplot)

# Package required to call 'corrplot' function
library(corrplot)

# Package required to call 'rplot' function
library(corrr)

# Package required to call 'ggplot' function (graphical tools)
library(ggplot2)

# Package required to call 'ggarrange' function (graphical tools)
library(ggpubr)

# Package required to call 'fviz_pca_var, fviz_pca_ind and fviz_pca' functions
library(factoextra)

# Package required to call 'scatterplot3d' function
library(scatterplot3d)

# Package required to call 'mutate' function
library(tidyverse)

# Package required to call 'clusGap' function
library(cluster)

# Package required to call 'ggdendrogram' function
library(ggdendro)

# Package required to call 'grid.arrange' function
library(gridExtra)

# Package required to call 'melt' function
library(reshape2)

# Package required to call 'mvn' function
library(MVN)

# Package required to call 'boxM' function
library(biotools)

# Package required to call 'partimat' function
library(klaR)

# Package required to call 'summarise' function
library(dplyr)

# Package required to call 'createDataPartition' function
library(caret)

```

## Descripción del conjunto de datos *student_mat_DB*

El fichero *student_mat_DB* contiene datos recogidos de 395 estudiantes recogidos en 31 variables. A continuación se muestra una descripción de cada variable del conjunto de datos:

- Centro escolar del estudiante (*school*)
- Sexo del estudiante (*sex*)
- Edad del estudiante (*age*)
- Tipo de dirección de vivienda del estudiante (*address*)
- Número de miembros de la familia del estudiante (*famsize*)
- Estado de convivencia de los padres del estudiante (*Pstatus*)
- Educación de la madre del estudiante (*Medu*)
- Educación del padre del estudiante (*Fedu*)
- Trabajo de la madre del estudiante (*Mjob*)
- Trabajo del padre del estudiante (*Fjob*)
- Motivo del estudiante por el cual eligió su centro escolar (*reason*)
- Tutor del estudiante (*guardian*)
- Tiempo de viaje de casa del estudiante al centro escolar (*traveltime*)
- Tiempo de estudio semanal del estudiante (*studytime*)
- Número de materias suspendidas en el pasado (*failures*)
- El estudiante recibe apoyo educativo adicional (*schoolsup*)
- El estudiante recibe apoyo educativo familiar (*famsup*)
- El estudiante recibe clases extras pagadas de la asignatura Matemáticas (*paid*)
- El estudiante está apuntado en actividades extracurriculares (*activities*)
- El estudiante asistió de pequeño a la guardería (*nursery*)
- El estudiante quiere cursar estudios superiores (*higher*)
- El estudiante tiene acceso a Internet en casa (*internet*)
- El estudiante se encuentra en una relación de pareja (*romantic*)
- Calidad de las relaciones familiares del estudiante (*famrel*)
- Cantidad de tiempo libre que posee el estudiante después del colegio (*freetime*)
- Frecuencia con la que el estudiante sale a la calle con sus amigos (*goout*)
- Cantidad de consumo de alcohol del estudiante en jornada laboral (*Dalc*)
- Cantidad de consumo de alcohol del estudiante en el fin de semana (*Walc*)
- Estado de salud actual del estudiante (*health*)
- Número de veces que el estudiante ha faltado al colegio (*absences*)
- Nota final del estudiante en la asignatura Matemáticas (*G3*)

### Lectura del conjunto de datos

El archivo de datos tiene extensión *.xlsx* (*Microsoft Excel*). Para cargar el fichero se utiliza la función *read_excel()* dentro del paquete *readxl*.

Para cargar la base de datos, la sesión de R debe ejecutarse en el mismo directorio en el que se encuentra el fichero. Para ello, se puede usar la función *setwd(“Ubicación del archivo de datos”)* o se puede pulsar *Session/Set Working Directory/Choose Directory* en la barra de menús de RStudio. Asumiendo que la sesión de R está correctamente direccionada, el siguiente código carga el fichero de datos en un data.frame.

```{r warning=FALSE, message=FALSE}

# Setting the work directory
setwd("C:/Users/LENOVO/Desktop/EMV/practicas/practica_final")

# Loading a .xlsx (excel) file.
# The output of this function is already a data.frame object
# Remember that package 'readxl' is required
data_xlsx<-read_excel("student_mat_DB.xlsx", sheet = 2)

# This sentence identifies the type of object that the identifier represents
class(data_xlsx)

```

# **Análisis Explotario Univariante**

## Recodificaciones o agrupaciones de datos

Se ha recodificado el conjunto de datos original. Esta recodificación ya se encuentra realizada y detallada en el archivo *student_mat_DB.xlsx*.

Cabe destacar que es cierto que, por el carácter de las variables, principalmente categóricas pero recodificadas, podrían discutirse los resultados obtenidos tras aplicar las técnicas para la reducción de la dimensión (ACP y AF, los cuales realizaremos posteriormente) puesto que estos trabajan con variables cuantitativas, pero también es cierto que si las variables son binarias (como ocurre en varias variables de nuestro conjunto de datos), todavía se puede justificar su uso y usabilidad. 

Hay que **tener cuidado con las variables codificadas que eran categóricas originalmente y que tienen más de dos niveles de respuesta** porque quizá no interese incluirlas. Para decidir si las incluímos o no, tendría que ser el que ha diseñado el dataset el que nos dijera si esas variables son relevantes para el problema bajo estudio. Asumiendo que lo son, una forma de ver si pueden ser útiles es hacer un análisis descriptivo de ellas y **ver si los niveles están compensados** (más o menos hay el mismo número de individuos en cada uno). Si lo están es una variable que podría aportar información, pero si hay algún nivel muy bajo o si todo se acumula en un nivel, habría que plantearse descartar la variable. No hay una forma definitiva de actuar, depende de lo que busquemos, pero este camino podría ayudar.

### Variable *Medu*

```{r warning=FALSE, message=FALSE}

# Frequency tables. Descriptive analysis
# Remember that package 'summarytools' is required
freq(as.factor(data_xlsx$Medu))

```

```{r warning=FALSE, message=FALSE}

# Pie chart and bar graph
p1<-ggplot(data_xlsx, aes(x=factor(1),fill=as.factor(data_xlsx$Medu)))+geom_bar()+
  coord_polar("y")+labs(x="Medu", y="%")

p2<-ggplot(data_xlsx, aes(x=factor(1),fill=as.factor(data_xlsx$Medu)))+geom_bar()+
  labs(x="Medu", y="%")

# This function controls the graphical output device
# Remember that package 'ggpubr' is required
ggarrange(p1, p2, nrow = 1, ncol=2, common.legend = TRUE)

```

Vemos que la frecuencia del nivel asociado al valor 0 es muy baja (no están compensados los niveles). Por tanto, eliminamos dicha variable de nuestro conjunto de datos.

```{r warning=FALSE, message=FALSE}

# The variable 'Medu' is eliminated
data_xlsx<-data_xlsx[, -c(7)]

```

### Variable *Fedu*

```{r warning=FALSE, message=FALSE}

# Frequency tables. Descriptive analysis
# Remember that package 'summarytools' is required
freq(as.factor(data_xlsx$Fedu))

```

```{r warning=FALSE, message=FALSE}

# Pie chart and bar graph
p3<-ggplot(data_xlsx, aes(x=factor(1),fill=as.factor(data_xlsx$Fedu)))+geom_bar()+
  coord_polar("y")+labs(x="Fedu", y="%")

p4<-ggplot(data_xlsx, aes(x=factor(1),fill=as.factor(data_xlsx$Fedu)))+geom_bar()+
  labs(x="Fedu", y="%")

# This function controls the graphical output device
# Remember that package 'ggpubr' is required
ggarrange(p3, p4, nrow = 1, ncol=2, common.legend = TRUE)

```

Vemos que la frecuencia del nivel asociado al valor 0 es muy baja (no están compensados los niveles). Por tanto, eliminamos dicha variable de nuestro conjunto de datos.

```{r warning=FALSE, message=FALSE}

# The variable 'Fedu' is eliminated
data_xlsx<-data_xlsx[, -c(7)]

```

### Variable *Mjob*

```{r warning=FALSE, message=FALSE}

# Frequency tables. Descriptive analysis
# Remember that package 'summarytools' is required
freq(as.factor(data_xlsx$Mjob))

```

```{r warning=FALSE, message=FALSE}

# Pie chart and bar graph
p5<-ggplot(data_xlsx, aes(x=factor(1),fill=as.factor(data_xlsx$Mjob)))+geom_bar()+
  coord_polar("y")+labs(x="Mjob", y="%")

p6<-ggplot(data_xlsx, aes(x=factor(1),fill=as.factor(data_xlsx$Mjob)))+geom_bar()+
  labs(x="Mjob", y="%")

# This function controls the graphical output device
# Remember that package 'ggpubr' is required
ggarrange(p5, p6, nrow = 1, ncol=2, common.legend = TRUE)

```

Vemos que más o menos están compensados los niveles de esta variable. Por tanto, mantenemos dicha variable en nuestro conjunto de datos.

### Variable *Fjob*

```{r warning=FALSE, message=FALSE}

# Frequency tables. Descriptive analysis
# Remember that package 'summarytools' is required
freq(as.factor(data_xlsx$Fjob))

```

```{r warning=FALSE, message=FALSE}

# Pie chart and bar graph
p7<-ggplot(data_xlsx, aes(x=factor(1),fill=as.factor(data_xlsx$Fjob)))+geom_bar()+
  coord_polar("y")+labs(x="Fjob", y="%")

p8<-ggplot(data_xlsx, aes(x=factor(1),fill=as.factor(data_xlsx$Fjob)))+geom_bar()+
  labs(x="Fjob", y="%")

# This function controls the graphical output device
# Remember that package 'ggpubr' is required
ggarrange(p7, p8, nrow = 1, ncol=2, common.legend = TRUE)

```

Vemos que la frecuencia del nivel asociado al valor 4 es muy alta (no están compensados los niveles). Por tanto, eliminamos dicha variable de nuestro conjunto de datos.

```{r warning=FALSE, message=FALSE}

# The variable 'Fjob' is eliminated
data_xlsx<-data_xlsx[, -c(8)]

```

### Variable *reason*

```{r warning=FALSE, message=FALSE}

# Frequency tables. Descriptive analysis
# Remember that package 'summarytools' is required
freq(as.factor(data_xlsx$reason))

```

```{r warning=FALSE, message=FALSE}

# Pie chart and bar graph
p9<-ggplot(data_xlsx, aes(x=factor(1),fill=as.factor(data_xlsx$reason)))+geom_bar()+
  coord_polar("y")+labs(x="reason", y="%")

p10<-ggplot(data_xlsx, aes(x=factor(1),fill=as.factor(data_xlsx$reason)))+geom_bar()+
  labs(x="reason", y="%")

# This function controls the graphical output device
# Remember that package 'ggpubr' is required
ggarrange(p9, p10, nrow = 1, ncol=2, common.legend = TRUE)

```

Vemos que más o menos están compensados los niveles de esta variable. Por tanto, mantenemos dicha variable en nuestro conjunto de datos.

### Variable *guardian*

```{r warning=FALSE, message=FALSE}

# Frequency tables. Descriptive analysis
# Remember that package 'summarytools' is required
freq(as.factor(data_xlsx$guardian))

```

```{r warning=FALSE, message=FALSE}

# Pie chart and bar graph
p11<-ggplot(data_xlsx, aes(x=factor(1),fill=as.factor(data_xlsx$guardian)))+geom_bar()+
  coord_polar("y")+labs(x="guardian", y="%")

p12<-ggplot(data_xlsx, aes(x=factor(1),fill=as.factor(data_xlsx$guardian)))+geom_bar()+
  labs(x="guardian", y="%")

# This function controls the graphical output device
# Remember that package 'ggpubr' is required
ggarrange(p11, p12, nrow = 1, ncol=2, common.legend = TRUE)

```

Vemos que la frecuencia del nivel asociado al valor 0 es muy alta (no están compensados los niveles). Por tanto, eliminamos dicha variable de nuestro conjunto de datos.

```{r warning=FALSE, message=FALSE}

# The variable 'guardian' is eliminated
data_xlsx<-data_xlsx[, -c(9)]

```

## Explorando el conjunto de datos

```{r warning=FALSE, message=FALSE}

# Retrieving the name of all variables
colnames(data_xlsx)

# Displaying a few records
head(data_xlsx, n=10)

data_xlsx

```

```{r warning=FALSE, message=FALSE}

# Checking the type of the variables
sapply(data_xlsx, class)

```

## Valores perdidos (NA)

```{r warning=FALSE, message=FALSE}

# Construction of the function that calculates the percentage of missing values for each variable
porcentaje_NA<-function(data){
  c=(sum(is.na(data)))/length(data)*100
  return(c)
}

# Checking for missing values
cont_NA<-apply(data_xlsx, 2, porcentaje_NA)
cont_NA

```

Se observa que la variable *Mjob* tiene menos de un 5% de valores perdidos, a diferencia de la variable *studytime*, que sí que tiene más de un 5%. Por lo tanto, hay que analizar en dicha variable *studytime* si el patrón seguido por dichos valores perdidos es aleatorio o no. Para ello, se estudia la homogeneidad según grupos (NA y no
NA) con otras variables según un Test de Student (ya que la variable es continua).

Se define una nueva variable *studytime_NA* que toma el valor 0 si el correspondiente dato no es perdido y el valor 1 si el correspondiente dato es perdido. A continuación, se realiza un test de contraste de igualdad de medias entre los grupos de la variable *traveltime* (que no tiene valores perdidos) correspondientes al 0 y al 1 de estas nuevas variables.

```{r warning=FALSE, message=FALSE}

# Construction of the function that replaces missing values of a variable with 1 and non-missing values with 0
indicadora_NA<-function(data, na.rm=F){
  data[!is.na(data)]<-0
  data[is.na(data)]<-1
  return(data)
}

studytime_NA<-indicadora_NA(data_xlsx$studytime)

# Test de Student
t.test(data_xlsx$traveltime~studytime_NA, var.equal=TRUE)

```

Como el p-valor es grande en todos los casos (p-valor > 0.15), no se encuentran evidencias para rechazar la hipótesis nula de igualdad de medias, de forma que se acepta la hipótesis de homogeneidad y se concluye que **el patrón es aleatorio**. En este caso, como las variables son cuantitativas, **la decisión para los datos perdidos es reemplazarlos por la media de su variable**.

```{r warning=FALSE, message=FALSE}

# Construction of the function that handles missing values
not_available<-function(data, na.rm=F){
  data[is.na(data)]<-mean(data, na.rm=T)
  return(data)
}

data_xlsx$Mjob<-not_available(data_xlsx$Mjob)
data_xlsx$studytime<-not_available(data_xlsx$studytime)

# Viewing the data again
head(data_xlsx, n=10)

```

```{r warning=FALSE, message=FALSE}

# Checking for missing values
no_cont_NA<-apply(data_xlsx, 2, porcentaje_NA)
no_cont_NA

```

## Análisis descriptivo numérico clásico

A continuación, se realiza un análisis descriptivo de los datos. La función *stat.desc()* del paquete *pastecs* proporciona las medidas de posición, dispersión y forma más importantes:

- Número de valores (*nbr.val*)
- Número de valores nulos (*nbr.null*)
- Número de valores perdidos (*nbr.na*)
- Valor mínimo de la variable (*min*)
- Valor máximo de la variable (*max*)
- Rango de valores de la variables (*max-min*) (*range*)
- Suma de los valores no perdidos (*sum*).
- Mediana (*median*).
- Media (*mean*).
- Error estándar de la media (*SE.mean*)
- Intervalo de confianza de la media (*CI.mean*)
- Varianza (*var*)
- Desviación típiaca (*std.dev*)
- Coeficiente de variación (*coef.var*)
- Coeficiente de asimetría (*skewness*)
- Estadístico para contrastar si el coeficiente de asimetría es cero (*skew.2SE*)
- Coeficiente de curtosis (*kurtosis*)
- Estadístico para contrastar si el coeficiente de curtosis es cero (*kurt.2SE*).
- Estadístico de Shapiro-Wilks (*normtest.W*)
- Probabilidad asociada al estadístico de Shapiro-Wilks (*normtest.p*)

```{r warning=FALSE, message=FALSE}

# Basic descriptive statistics
stat.desc(data_xlsx, norm=TRUE)

```

Para más detalles sobre esta función se puede consultar el siguiente enlace:

https://www.rdocumentation.org/packages/pastecs/versions/1.3.21/topics/stat.desc

Como esta función no muestra los cuartiles, se utiliza la función *summary()* para obtenerlos junto al resto de medidas de posición:

```{r warning=FALSE, message=FALSE}

# Basic descriptive statistics
summary(data_xlsx)

```

## Outliers

Para detectar los **valores atípicos (outliers)**, hay que hacer un análisis exploratorio gráfico previo, construyendo los *boxplots* de todas las variables.

```{r warning=FALSE, message=FALSE}

# Boxplots of all variables together
# This visualization is not the best due to the difference between the scales
boxplot(data_xlsx, main="Outliers",
        xlab="All explanatory variables",
        ylab="Values",
        col=c(1:ncol(data_xlsx)))

```

**Se ha tomado la decisión de sustituir los outliers por la media de sus variables**. Para ello, se ha construido la función *outlier()*, que realiza la detección y manipulación de los mismos. Cabe destacar que la decisión de sustituir los outliers por la media requeriría un análisis previo de la causa de estos valores atípicos.

```{r warning=FALSE, message=FALSE}

# Recursive function that modifies outliers by the mean of their variable
outlier<-function(data, na.rm=T){
  H<-1.5*IQR(data)
  data[data<quantile(data, 0.25, na.rm = T)-H]<-NA
  data[data>quantile(data, 0.75, na.rm = T)+H]<-NA
  data[is.na(data)]<-mean(data, na.rm = T)

  H<-1.5*IQR(data)

  if (TRUE %in% (data<quantile(data, 0.25, na.rm = T)-H) |
      TRUE %in% (data>quantile(data, 0.75, na.rm = T)+H))
    outlier(data)
  else
    return(data)
}

# This data.frame is to preserve original data once the outliers are modified
original_data_xlsx<-data_xlsx

# Called to outlier function for each variable identified with outliers
data_xlsx<-as.data.frame(apply(data_xlsx, 2, outlier))

# We compare the original data and the fixed ones with respective boxplots 
par(mfrow=c(1,2))
# Boxplot original data
boxplot(original_data_xlsx, main="Original data",
        xlab="All explanatory variables",
        ylab="Values",
        col=c(1:ncol(original_data_xlsx)))
# Boxplot fixed data
boxplot(data_xlsx, main="Data with no outliers",
        xlab="All explanatory variables",
        ylab="Values",
        col=c(1:ncol(data_xlsx)))

```

Vemos que se aprecian muy bien los boxplots debido a los altos valores de la penúltima variable (*absences*) en comparación con el resto. A continuación se repite el análisis de outliers pero con los datos normalizados para que se pueda tener una mejor visualización de las gráficas anteriores.

```{r warning=FALSE, message=FALSE}

# Normalized original data
normalized_data_xlsx<-original_data_xlsx
normalized_data_xlsx<-scale(original_data_xlsx)

# Boxplots of all variables together
# This visualization is not the best due to the difference between the scales
boxplot(normalized_data_xlsx, main="Outliers",
        xlab="All explanatory variables",
        ylab="Values",
        col=c(1:ncol(normalized_data_xlsx)))

```

```{r warning=FALSE, message=FALSE}

# This data.frame is to preserve original data once the outliers are modified
normalized_original_data_xlsx<-normalized_data_xlsx

# Called to outlier function for each variable identified with outliers
normalized_data_xlsx<-as.data.frame(apply(normalized_data_xlsx, 2, outlier))

# We compare the normalized original data and the fixed ones with respective boxplots 
par(mfrow=c(1,2))
# Boxplot normalized original data
boxplot(normalized_original_data_xlsx, main="Normalized original data",
        xlab="All explanatory variables",
        ylab="Values",
        col=c(1:ncol(normalized_original_data_xlsx)))
# Boxplot fixed data
boxplot(normalized_data_xlsx, main="Normalized data with no outliers",
        xlab="All explanatory variables",
        ylab="Values",
        col=c(1:ncol(normalized_data_xlsx)))

```

# **Análisis de Componentes Principales (ACP)**

## Correlación

En primer lugar es necesario comprobar que las variables no son independientes. A nivel de la muestra recogida en la base de datos, esto se puede hacer calculando y observando la matriz de correlaciones. A nivel poblacional se justificará que hay correlación realizando el **test de Bartlett** (el *contraste de esfericidad de Bartlett* permite comprobar si las correlaciones son distintas de 0 de modo significativo. La hipótesis nula es que $det(R)=1$
, siendo $R$ la matriz de correlaciones). El siguiente código realiza las dos comprobaciones indicadas.

```{r warning=FALSE, message=FALSE}

###############################
# Correlation at sample level #
###############################

# Are the variables correlated at sample level?
correlation_matrix<-cor(data_xlsx)
correlation_matrix

```
Vemos que hay algunas filas en la matriz de correlación con el valor 'NA' correspondientes a las variables *school*, *address*, *Pstatus*, *failures*, *schoolsup*, *nursery*, *higher* e *internet*. Esto se debe a que todos los registros de estas variables tienen el mismo valor (en este caso esto se debe al tratamiento de los outliers), haciendo que su desviación típica sea igual a 0 y, por lo tanto, no podemos calcular su correlación con otras variables (ya que sería dividir por 0, lo cual no es posible). Por tanto, debido a que disponemos de muchas más variables, se ha decidido eliminar estas variables.

```{r warning=FALSE, message=FALSE}

# The variables 'school', 'address', 'Pstatus', 'failures', 'schoolsup', 'nursery', 'higher' and 'internet' are eliminated
data_xlsx_reduced<-data_xlsx[, -c(1, 4, 6, 11, 12, 16, 17, 18)]

# The first three records in the database are displayed
head(data_xlsx_reduced, n=10)

```

En cualquier caso, para hacer el análisis de la correlación hemos trabajado con la matriz de correlaciones de todas las variables. Aquí hay un **problema** porque **estamos mezclando variables por un lado continuas o discretas, con variables categóricas que han sido codificadas (*sexo*, *school*, *address*, etc.)**. Es un problema porque por defecto estamos obteniendo las correlaciones de Pearson que trata todas las variables como cuantitativas y no lo son, llevando así a engaño. Para variables categóricas habría que cambiar el parámetro de la función de R para que obtuviera la correlación de Spearman. Pero claro, ya no podríamos hacer una reducción de la dimensión con todas a la vez. Por un lado lo haríamos con las cuantitativas que es para lo que funciona el ACP por su construcción y por otro lado investigaríamos como hacerlo con variables categóricas (lo cual no hemos visto). **Este comentario es solo para tenerlo en cuenta de cara al futuro. Para esta práctica, el procedimiento seguido mezclando todas las variables es suficiente.** En conclusión, **recordar para trabajos futuros que la reducción de la dimensión la hacéis con las variables del mismo tipo (cuantitativas)**.

De acuerdo con los resultados numéricos a continuación, se observa que los datos no están correlacionados ni a nivel muestral (ver matriz de correlación) ni a nivel poblacional (la prueba de esfericidad de Bartlett no es significativa).

```{r warning=FALSE, message=FALSE}

###############################
# Correlation at sample level #
###############################

# Are the variables correlated at sample level?
correlation_matrix<-cor(data_xlsx_reduced)
correlation_matrix

det(correlation_matrix)

```

```{r warning=FALSE, message=FALSE}

###################################
# Correlation at population level #
###################################

# Bartlett's sphericity test:
# This test checks whether the correlations are significantly different from 0
# The null hypothesis is H_0; det(R)=1 means the variables are uncorrelated 
# R denotes the correlation matrix
# cortest.bartlett function in the package pysch performs this test
# This function works with standardized data.

# Standardization
data_xlsx_scale<-scale(data_xlsx_reduced)

# Bartlett's sphericity test
cortest.bartlett(cor(data_xlsx_scale))

```

A la vista de los resultados del test (p-value > 0.001), **no se puede rechazar que los datos sean incorrelados, es decir, que sean independientes**. Por tanto, **no tendría mucho sentido realmente plantearse una reducción de la dimensión mediante el procedimiento de Análisis de Componentes Principales (ACP) o Análisis Factorial (AF)**. Sin embargo, vamos a realizarlo con el fin de mostrar cómo se haría.

## Realización del ACP

El siguiente código realiza el ACP, obteniendo los vectores propios que generan cada componente, así como sus valores propios, que corresponden a la varianza de cada una.

```{r warning=FALSE, message=FALSE}

# The 'prcomp' function in the base R package performs this analysis
# Parameters 'scale' and 'center' are set to TRUE to consider standardized data
PCA<-prcomp(data_xlsx_reduced, scale=T, center=T)

# The field 'rotation' of the 'PCA' object is a matrix 
# Its columns are the coefficients of the principal components
# Indicates the weight of each variable in the corresponding principal component
PCA$rotation

```

```{r warning=FALSE, message=FALSE}

# Standard deviations of each principal component
PCA$sdev

```

Cada componente principal se obtiene de forma sencilla como una combinación lineal de todas las variables con los coeficientes indicados por las columnas de la matriz de rotación.

## Tasa de variación explicada

```{r warning=FALSE, message=FALSE}
# The function 'summary' applied to the 'PCA' object provides relevant information
# - Standard deviations of each principal component
# - Proportion of variance explained and cummulative variance
summary(PCA)

```

Los siguientes gráficos ilustran el comportamiento de la varianza explicada por cada componente principal, así como el comportamiento de la varianza explicada acumulada.

```{r warning=FALSE, message=FALSE}

# The following graph shows the proportion of explained variance
Explained_variance <- PCA$sdev^2 / sum(PCA$sdev^2)

p13<-ggplot(data = data.frame(Explained_variance, pc = 1:ncol(data_xlsx_reduced)),
  aes(x = pc, y = Explained_variance, fill=Explained_variance )) +
  geom_col(width = 0.3) + scale_y_continuous(limits = c(0,0.6)) + theme_bw() +
  labs(x = "Principal component", y= "Proportion of variance")

# The following graph shows the proportion of cumulative explained variance
Cummulative_variance<-cumsum(Explained_variance)

p14<-ggplot( data = data.frame(Cummulative_variance, pc = 1:ncol(data_xlsx_reduced)),
  aes(x = pc, y = Cummulative_variance ,fill=Cummulative_variance )) +
  geom_col(width = 0.5) +  scale_y_continuous(limits = c(0,1)) + theme_bw() +
  labs(x = "Principal component",
       y = "Proportion of cumulative variance")

p13
p14
```

## Número de componentes principales apropiado

Para elegir el número adecuado de componentes principales, existen diferentes métodos. Nosotros vamos a utilizar la **Regla de Abdi et al. (2010)**: Se promedian las varianzas explicadas por las componentes principales y se seleccionan aquellas cuya proporción de varianza explicada supera la media.

```{r warning=FALSE, message=FALSE}

#######################
# Rule of Abdi et al. #
#######################

# Variances
PCA$sdev^2

```

```{r warning=FALSE, message=FALSE}

# Average of variances
mean(PCA$sdev^2)

```

A la vista de los resultados, en este caso, se eligen **ocho direcciones principales** que, tal y como se puede ver, acumulan alrededor del 60% de varianza explicada.

## Salidas gráficas de interés

Los siguientes gráficos muestran la **comparativa entre distintas componentes principales** mediante una proyección al plano sobre cada dos componentes. En esta representación se aprecia cuáles de las variables originales tienen mayor o menor peso en cada una de las componentes enfrentadas.

```{r warning=FALSE, message=FALSE}

# These graphical outputs show the projection of the variables in two dimensions
# Display the weight of the variable in the direction of the principal component 
p15<-fviz_pca_var(PCA, repel=TRUE, col.var="cos2",
                 legend.title="Distance", title="Variables")+theme_bw()

p16<-fviz_pca_var(PCA, axes=c(1,3), repel=TRUE, col.var="cos2",
                 legend.title="Distance", title="Variables")+theme_bw()

p17<-fviz_pca_var(PCA, axes=c(2,3), repel=TRUE, col.var="cos2",
                 legend.title="Distance", title="Variables")+theme_bw()

# Displaying graphics
p15
p16
p17

```

Es posible también **representar las observaciones** de los objetos junto con las componentes principales mediante la orden *contrib()* de la función *fviz_pca_ind()* anterior, así como identificar con colores aquellas observaciones que mayor varianza explican.

```{r warning=FALSE, message=FALSE}

# It is also possible to represent the observations
# As well as identify with colors those observations that explain the greatest 
# variance of the principal components
p18<-fviz_pca_ind(PCA,col.ind = "contrib", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel=TRUE,legend.title="Contrib.var", title="Records")+theme_bw()

p19<-fviz_pca_ind(PCA,axes=c(1,3),col.ind = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel=TRUE,legend.title="Contrib.var", title="Records")+theme_bw()

p20<-fviz_pca_ind(PCA,axes=c(2,3),col.ind = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel=TRUE,legend.title="Contrib.var", title="Records")+theme_bw()

# Displaying graphics
p18
p19
p20

```

Finalmente, se puede obtener una **representación conjunta de variables y observaciones** que relaciona visualmente las posibles relaciones entre las observaciones, las contribuciones de los individuos a las varianzas y el peso de las variables en cada componente principal.

```{r warning=FALSE, message=FALSE}

# Joint representation of variables and observations
# Relates the possible relationships between the contributions of the records
# to the variances of the components and the weight of the variables in each 
# principal component

p21<-fviz_pca(PCA,alpha.ind ="contrib", col.var = "cos2",
         col.ind="seagreen",
         gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
         repel=TRUE, legend.title="Distancia")+theme_bw()

p22<-fviz_pca(PCA,axes=c(1,3),alpha.ind ="contrib", 
         col.var = "cos2",col.ind="seagreen",
         gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
         repel=TRUE, legend.title="Distancia")+theme_bw()

p23<-fviz_pca(PCA,axes=c(2,3),alpha.ind ="contrib", 
         col.var = "cos2",col.ind="seagreen",
         gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
         repel=TRUE, legend.title="Distancia")+theme_bw()

# Displaying graphics
p21
p22
p23

```

Finalmente, dado que el objeto de este estudio es reducir la dimensión del conjunto de datos, es posible obtener las coordenadas de los datos originales en el nuevo sistema de referencia. De hecho, se almacenan desde que usamos la función *prcomp()* para crear la variable PCA.

```{r warning=FALSE, message=FALSE}

head(PCA$x)

```

Recordemos que **no hemos podido rechazar que los datos sean incorrelados, es decir, que sean independientes**. Por tanto, **la reducción de la dimensión mediante el procedimiento de Análisis de Componentes Principales (ACP) o Análisis Factorial (AF) no tendría mucho sentido realmente**.

# **Análisis Factorial (AF)**

## Correlación

Es necesario comprobar que las variables no son independientes. Ya vimos en el apartado 3, al comprobar la correlación de las variables para el Análisis de Componentes Principales, que las variables sí son independientes tras calcular la matriz de correlación y realizar el test de Bartlett. Esto nos dice, volvemos a repetirlo, qque **la reducción de la dimensión mediante el procedimiento de Análisis de Componentes Principales (ACP) o Análisis Factorial (AF) no tendría mucho sentido realmente**. Sin embargo, igual que hicimos en el Análisis de Componentes Principales, vamos a realizarlo con el fin de mostrar cómo se haría.

Caben destacar también los siguientes resultados gráficos que proporcionan una idea intuitiva de la correlación entre las variables:

```{r warning=FALSE, message=FALSE}

# Polychoric correlation matrix
# Remember that package 'polycor' is required for 'hetcor' function
poly_cor<-hetcor(data_xlsx_reduced)$correlations

# Remember that package 'ggcoorplot' is required for 'ggcorrplot' function
ggcorrplot(poly_cor, type="lower", hc.order=T)

```

```{r warning=FALSE, message=FALSE}

# Another interesting visual representation is the following
# Remember that package 'corrplot' is required for 'corrplot' function
corrplot(cor(data_xlsx_reduced), order = "hclust", tl.col='black', tl.cex=1)

```

```{r warning=FALSE, message=FALSE}

# Or this other option is also very visual
# Remember that package 'corrr' is required for 'rplot' function
data_xlsx_reduced_correlations <- correlate(data_xlsx_reduced)
rplot(data_xlsx_reduced_correlations, legend = TRUE, colours = c("firebrick1", "black","darkcyan"), print_cor = TRUE) 

```

Tal y como era de esperar, podemos observar que las variables apenas están correladas entre sí (lo cual corrobora los resultados numéricos de la matriz de correlación y el test de Bartlett que obtuvimos anteriormente).

## Diferentes métodos de estimación

Para extraer los factores hay que escoger un método de estimación. Para ello, se utiliza la función *fa()*, que permite implementar hasta 6 métodos distintos.

A continuación, se comparan las salidas com el *método del factor principal* y con el *método de máxima verosimilitud*. Se calcula 1 factor, ya que es lo que se ve intuitivamente en los mapas de correlaciones anteriores.

```{r warning=FALSE, message=FALSE}

### Test of two models with three factors
modelo1<-fa(poly_cor,
            nfactors = 1,
            rotate = "none",
            fm="mle") # likelihood method

modelo2<-fa(poly_cor,
            nfactors = 1,
            rotate = "none",
            fm="minres") # minimal residual model

# Outputs of these models: factorial matrices, etc.

print("Modelo 1: mle")
modelo1$loadings

print("Modelo 2: minres")
modelo2$loadings

```

Se comparan las **comunalidades** de ambos modelos para ver qué proporción de la varianza es explicada por los factores latentes.

```{r warning=FALSE, message=FALSE}

# Comparing communalities
sort(modelo1$communality, decreasing = T)->c1
sort(modelo2$communality, decreasing = T)->c2
head(cbind(c1, c2))

```

También se comparan las **unicidades**, esto es, la proporción de varianza que no ha sido explicada por el factor (1-comunalidad):

```{r warning=FALSE, message=FALSE}

# Comparison of uniqueness, that is, the proportion of variance that has not 
# been explained by the factor (1-communality)
sort(modelo1$uniquenesses, decreasing = T)->u1
sort(modelo2$uniquenesses, decreasing = T)->u2
head(cbind(u1,u2), n=10)

```

Se observa que con el método de máxima verosimilitud se obtienen valores un poco más altos, luego los factores latentes obtenidos con este método explican mejor la varianza de las variables observadas.

## Número de factores latentes apropiado

Existen diferentes criterios, entre los que destacan el **scree plot** (Cattel 1966) y el **análisis paralelo** (Horn 1965).

El **método del codo** consiste en representar el gráfico de sedimentación, que se obtiene al representar en las ordenadas los valores propios en orden decreciente y en las abscias los números correspondiente a las componentes principales o factores latentes (según se esté haciendo un ACP o un AF). Uniendo los puntos se obtiene una figura que empieza con fuerte pendiente para luego seguir con una ligera inclinación. El método indica que hay que quedarse con el número de factores que haya hasta el “codo” de la gráfica, donde se pasa de la fuerte pendiente a la ligera inclinación.

Se llama **gráfico de sedimentación** (scree plot en inglés), ya que parece el perfil de una montaña con una gran pendiente hasta llegar a la base, que es una meseta donde se acumulan los guijarros caídos desde la cumbre (donde sedimentan).

Según los siguientes resultados gráficos, **1** se considera el **número óptimo de factores latentes** (análisis paralelo), coincidiendo con el primer gráfico scree plot.

```{r warning=FALSE, message=FALSE}

# Scree plot
scree(poly_cor)

```

```{r warning=FALSE, message=FALSE}

# Parallel analysis
fa.parallel(poly_cor, n.obs=100, fa="fa", fm="ml")

```

Otra forma de estudiar el número óptimo de factores es con el siguiente test de hipótesis, que contrasta si el número de factores es suficiente o no:

```{r warning=FALSE, message=FALSE}
# Remember that package 'stats' is required for 'factanal' function
# This function only performs the mle method
factanal(data_xlsx_reduced, factors=1, rotation="varimax")

```
Así, se acepta la hipótesis de que 1 factor es suficiente.

## Realización del AF

Finalmente, se estima el modelo factorial con 1 factor implementando una rotación de tipo varimax para buscar una interpretación más simple de la realidad.

```{r warning=FALSE, message=FALSE}

modelo_varimax<-fa(poly_cor, nfactors=1, rotate="varimax", fa="mle")

# The rotated factorial matrix is shown
print(modelo_varimax$loadings, cut=0) 

```

Visualmente se podría hacer el esfuerzo de ver con qué variables correlaciona cada uno de los factores en la matriz factorial, pero esto es muy tedioso, por lo que se utiliza la siguiente representación:

```{r warning=FALSE, message=FALSE}

fa.diagram(modelo_varimax)

```

En el diagrama se observa que el único factor latente que tenemos correlaciona con las variables *Walc*, *Dalc*, *goout* y *sex*, siendo un resultado muy lógico que era de esperar.

Vemos que solo tenemos un factor latente y que solo correlaciona cuatro variables. Esto se debe a que (lo volvemos a recordar por última vez) **no hemos podido rechazar que los datos sean incorrelados, es decir, que sean independientes**. Por tanto, **la reducción de la dimensión mediante el procedimiento de Análisis de Componentes Principales (ACP) o Análisis Factorial (AF) no tendría mucho sentido realmente**.

# **Análisis Discriminante Lineal (ADL)**

La base de datos contiene información sobre la nota final del estudiante en la asignatura Matemáticas. Así, sería interesante dar un método de clasificación para la calificación según los demás indicadores.

Para ello, haremos que la variable *G3* sea categórica, con dos categorías:

- Aprobado si la nota dada en la variable *G3* es mayor o igual que 10.

- Suspenso si la nota dada en la variable *G3* es menor que 10.

```{r warning=FALSE, message=FALSE}

calification = c()

for(k in 1:nrow(data_xlsx)) {
  if(data_xlsx_reduced$G3[k] >= 10){
    calification[k] = "aprobado"
  }
  else
    calification[k] = "suspenso"
}

calification<-as.factor(calification)

# 'G3' now is a cathegorical variable
data_xlsx_G3_cathegorical = data_xlsx_reduced
data_xlsx_G3_cathegorical$G3 = calification

data_xlsx_G3_cathegorical$G3

```

Por otro lado, sabemos que el AD se basa en el concepto de distancia de Mahalanobis y se utiliza comúnmente para clasificar casos en grupos. Para realizar un AD, necesitamos al menos una variable categórica que actúe como tu **variable dependiente** (esta es la variable *data_xlsx_G3_cathegorical$G3* en nuestro caso) y varias variables independientes numéricas que se usarán para predecir el grupo de clasificación.

Las variables categóricas binarias también pueden ser incluidas como variables independientes en el AD si se codifican adecuadamente, aunque su interpretación puede ser menos clara que con las variables continuas. De esta forma, **para las variables independientes, vamos a considerar las variables cuantitativas y cualitativas que están codificadas numéricamente.**

Vamos a trabajar con una **partición del dataset, como conjunto de entrenamiento (con aproximadamente el 80% de los registros)**, sobre el que realizaremos el análisis discriminante y **otra partición, como conjunto test (con el 20% de los registros restantes)**, con el que realizaremos la validación del modelo. Usaremos el dataset *data_xlsx_G3_cathegorical* definido a partir de *data_xlsx_reduced* (en los histogramas de la exploración gráfica de datos que se realiza a continuación se justifica el uso de este en vez del original).

```{r warning=FALSE, message=FALSE}


# Partitioning the dataset: training (80%) + test (20%)
train_index<-createDataPartition(data_xlsx_G3_cathegorical$G3, p=0.80)$Resample1

train_data<-data_xlsx_G3_cathegorical[train_index,]
test_data<-data_xlsx_G3_cathegorical[-train_index,]

```

## Exploración gráfica de los datos

En primer lugar exploramos como de bien (o mal) clasifica en las calificaciones **cada una de las variables explicativas** consideradas de forma independiente. Para ello, dibujamos los histogramas superpuestos. Si **los histogramas se separan**, la variable considerada sería un **buen clasificador individual** para la calificación.

```{r warning=FALSE, message=FALSE}


# Variables list
variables <- c("sex", "age", "famsize", "Mjob", "reason", "traveltime",
               "studytime", "famsup", "paid", "activities", "romantic",
               "famrel", "freetime", "goout", "Dalc", "Walc", "health",
               "absences")

# Graphics list
graphics <- list()

for (var in variables) {
  p <- ggplot(data = train_data, aes_string(x = var, fill = "G3")) +
       geom_bar(position = "identity", alpha = 0.5)
  graphics[[var]] <- p
}

ggarrange(plotlist = graphics, nrow = 5, ncol = 4, common.legend = TRUE)

```

En este sentido, a primera vista no se ve ninguna variable que diferencia muy bien la calificación.

Si hubiéramos usado el dataset  *data_xlsx*, hubiésemos visto que hay **histogramas de algunas variables con una sola barra**. Esto es porque dichas variables tienen el mismo valor para todos los registros (consecuencia del tratamiento de outliers), haciendo que su varianza sea cero (lo cual nos daría problemas para el Test de normalidad univariante de Shapiro-Wilks, el cual realizaremos posteriormente). En concreto, estas variables son *school*, *address*, *Pstatus*, *failures*, *schoolsup*, *nursery*, *higher* e *internet*. Por ello, **como tenemos un gran número de variables, se ha decidido eliminarlas**, tal y como hicimos en ACP y AF, resultando así el dataset que ya hemos definido anteriormente *data_xlsx_reduced* (el cual estamos utilizando).

A continuación, se explora qué **pares de variables** separan mejor entre calificaciones.

```{r warning=FALSE, message=FALSE}

pairs(x = train_data[, c("sex", "age", "famsize", "Mjob", "reason", "traveltime",
                         "studytime", "famsup", "paid", "activities", "romantic",
                         "famrel", "freetime", "goout", "Dalc", "Walc", "health",
                         "absences")
                    ],
      col = c("firebrick", "green3")[data_xlsx_G3_cathegorical$G3], pch = 19)

```

Vemos que al disponer de un número de variables elevado, no se aprecia apenas el gráfico, por lo que no podemos deducir a primera vista alguna pareja de variables que separe adecuadamente las calificaciones. Igualmente, no parece que la haya.

## Normalidad univariante y multivariante

### Normalidad univariante

A continuación se realiza una exploración gráfica de la **normalidad** de las **distribuciones univariantes** de nuestros predictores representando los **histogramas** y los **gráficos qqplots**.

**Histogramas univariantes**

```{r warning=FALSE, message=FALSE}

# Histogram representation of each variable for each species
par(mfcol = c(2, 3))
for (k in 1:18) {
  j0 <- names(train_data)[k]
  x0 <- seq(min(train_data[, k]), max(train_data[, k]), le = 50)
  for (i in 1:2) {
    i0 <- levels(train_data$G3)[i]
    x <- train_data[train_data$G3 == i0, j0]
    hist(x, proba = T, col = grey(0.8), main = paste("G3", i0), xlab = j0)
    lines(x0, dnorm(x0, mean(x), sd(x)), col = "red", lwd = 2)
  }
}

```

**Gráficos qqplots**

```{r warning=FALSE, message=FALSE}

# Representation of normal quantiles of each variable for each species
par(mfcol = c(2, 3))
for (k in 1:18) {
  j0 <- names(train_data)[k]
  x0 <- seq(min(train_data[, k]), max(train_data[, k]), le = 50)
  for (i in 1:2) {
    i0 <- levels(train_data$G3)[i]
    x <- train_data[train_data$G3 == i0, j0]
    qqnorm(x, main = paste("G3", i0, j0), pch = 19, col = i + 1)
    qqline(x)
  }
}

```

Este análisis exploratorio puede darnos una idea de la posible distribución normal de las variables univariadas, pero siempre es mejor hacer los respectivos test de normalidad.

**Test de normalidad univariantes (Shapiro-Wilks)**

Se contrasta la **hipótesis nula** de que **los datos siguen una distribución normal** univariante. Se **rechaza** esta hipótesis si el **p-valor** dado por el test de Shapiro-Wilk es **menor que 0.05**. En otro caso no se rechaza el supuesto de normalidad de los datos.

```{r warning=FALSE, message=FALSE}

data_tidy <- melt(train_data, value.name = "valor")
aggregate(valor ~ G3 + variable, data = data_tidy,
          FUN = function(x){shapiro.test(x)$p.value})

```

**Se encuentran evidencias de falta de normalidad univariante** (p-valor < 0.05). A pesar de ello, se procede como en ACP y AF, es decir, tiramos para adelante con el fin de mostrar cómo se realizaría el análisis discriminante, pero habría que tener cuidado a la hora de obtener los resultados.

### Normalidad multivariante

**Test de normalidad multivariante (Mardia, Henze-Zirkler y Royston)**

El paquete *MVN* contiene funciones que permiten realizar los tres test que se utilizan habitualmente para **contrastar la normalidad multivariante**.

Esta normalidad multivariante puede verse afectada por la presencia de outliers multivariantes. En este paquete también encontramos funciones para el **análisis de outliers multivariantes**. Se haría de la siguiente forma:

$outliers <- mvn(data = train\_data[, -ncol(train\_data)], mvnTest = "hz", multivariateOutlierMethod = "quan")$

Si la ejecutamos obtenemos el error de que la matriz de correlaciones no tiene inversa ya que tiene determinante igual a cero, lo cual no es cierto como se puede ver a continuación.

```{r warning=FALSE, message=FALSE}

cor_m <- cor(train_data[, -ncol(train_data)])
det(cor_m)

```
De igual forma, los dos test realizados a continuación encuentran evidencias al 5% de significación de **falta de normalidad multivariante**.

```{r warning=FALSE, message=FALSE}

# Royston multivariate normality test
royston_test <- mvn(data = train_data[,-ncol(train_data)], mvnTest = "royston", multivariatePlot = "qq")

royston_test$multivariateNormality

```

```{r warning=FALSE, message=FALSE}

# Henze-Zirkler multivariate normality test
hz_test <- mvn(data = train_data[,-ncol(train_data)], mvnTest = "hz")
hz_test$multivariateNormality

```

Cabe destacar que **el discriminante lineal es muy sensible a la falta de normalidad multivariante y habría que leer sus resultados con cuidado, pero el cuadrático es bastante robusto frente a esta ausencia.**

## Homogeneidad de la varianza

Para el análisis de la homogeneidad de la varianza:

- Cuando hay **un solo predictor** el test más recomendable es el **test de Barttlet**.

- Cuando se emplean **múltiples predictores**, se tiene que contrastar que la matriz de covarianzas es constante en todos los grupos. En este caso es también recomendable comprobar la homogeneidad de la varianza para cada predictor a nivel individual. El test más recomendable es el **test de Box M**, que es una extensión del de Barttlet para escenarios multivariantes. Hay que tener en cuenta que **es muy sensible a que los datos efectivamente se distribuyan según una normal multivariante**. Por este motivo se recomienda utilizar una significación (p-value) < 0.001 para rechazar la hipótesis nula.

La **hipóstesis nula** a contrastar es la de **igualdad de matrices de covarianzas en todos los grupos**.

```{r warning=FALSE, message=FALSE}

boxM(data = train_data[, -19], grouping = train_data[, 19])

```

En este caso no se rechaza la hipótesis nula ya que *p-valor > 0.001* y por tanto **se asume la homogeneidad de varianzas**. Es importante recordar que para que **esta conclusión sea fiable** debe darse el supuesto de **normalidad multivariante (la cual vimos que no se daba en nuestro caso, pero aún así tiramos para adelante)**.

## Función discriminante

Aunque **no se satisfagan los supuestos de normalidad multivariante**, vamos a ajustar un **modelo de clasificación discriminante lineal** (tiramos para adelante con el fin de mostrar cómo se realizaría el análisis discriminante lineal, aunque hay que tenerlo presente ante la posiblidad de obtener resultados inesperados).

La función *lda()* del paquete *MASS* ajusta este modelo.

```{r warning=FALSE, message=FALSE}

lda_model <- lda(formula = G3 ~ sex + age + famsize + Mjob + reason + traveltime +
                                studytime + famsup + paid + activities + romantic + 
                                famrel + freetime + goout + Dalc + Walc + health +
                                absences,
                 data = train_data)
lda_model

```

La salida de este objeto, muestra las **probabilidades a priori de cada grupo**, en este caso 0.6708861 para *aprobado* y 0.3291139 para *suspenso*, las **medias de cada regresor por grupo** y los **coeficientes del modelo de clasificación discriminante lineal**.

Una vez construido el clasificador, se pueden clasificar nuevos datos en función de sus medidas sin más que llamar a la función *predict()*. Por ejemplo, **vamos a clasificar todas las observaciones del dataset test**.

```{r warning=FALSE, message=FALSE}

new_observations <- test_data

predict(object = lda_model, newdata = new_observations)

```

Por ejemplo, según la función discriminante, la probabilidad a posteriori de que el segundo registro del conjunto test  **apruebe** es del 89.6% mientras la de que **suspenda** es tan solo del 10.4%. Por tanto este dato sería clasificado en la calificación **aprobado**. Se razona del mismo modo con el resto de elementos del conjunto test.

## Validación del modelo

La función *confusionmatrix()* del paquete *biotools* realiza una validación cruzada del modelo de clasificación.

```{r warning=FALSE, message=FALSE}

pred <- predict(object = lda_model, newdata = test_data)
confusionmatrix(test_data$G3, pred$class)

```

```{r warning=FALSE, message=FALSE}

# Classification error percentage
test_error <- mean(test_data$G3 != pred$class) * 100
paste("test_error=", test_error, "%")

```

## Visualización de las clasificaciones

Desde el punto de vista geométrico, el análisis discriminante lineal separa el espacio mediante una recta. En este sentido, la función *partimat()* del paquete *klaR* permite representar los **límites de clasificación** de un modelo discriminante lineal o cuadrático para cada par de predictores. Cada color representa una región de clasificación acorde al modelo, **se muestra el centroide** de cada región y el **valor real de las observaciones**.

```{r warning=FALSE, message=FALSE}

partimat(G3 ~ sex + age + famsize + Mjob + reason + traveltime + studytime +
              famsup + paid + activities + romantic + famrel + freetime + 
              goout + Dalc + Walc + health + absences,
         data = test_data, method = "lda", prec = 200,
         image.colors = c("green", "orange"),
         col.mean = "yellow",nplots.vert =1, nplots.hor=3)

```

# **Análisis Discriminante Cuadrático (ADC)**

Al igual que para el Análisis Discriminante Lineal, para realizar el Análisis Discriminante Cuadrático se empieza con la **exploración gráfica de los datos** y las **comprobaciones sobre normalidad univariante y multivariante y homogeneidad de las varianzas**, que ya se han realizado previamente.

Vimos que no se verifica el supuesto de normalidad multivariante. Aun así, **se procede a ajustar un modelo discriminante cuadrático** porque es robusto frente a la falta de este supuesto, aunque hay que tenerlo presente ante la posiblidad de obtener resultados inesperados.

## Función discriminante

La función *qda()* del paquete *MASS* realiza la clasificación.

```{r warning=FALSE, message=FALSE}

qda_model <- qda(formula = G3 ~ sex + age + famsize + Mjob + reason + traveltime +
                                studytime + famsup + paid + activities + romantic + 
                                famrel + freetime + goout + Dalc + Walc + health +
                                absences,
                 data = train_data)
qda_model

```

La salida de este objeto, muestra las **probabilidades a priori de cada grupo**, en este caso 0.6708861 para *aprobado* y 0.3291139 para *suspenso*, las **medias de cada regresor por grupo**.

Una vez construido el clasificador, se pueden clasificar nuevos datos en función de sus medidas sin más que llamar a la función *predict()*. Por ejemplo, **vamos a clasificar todas las observaciones del dataset test**.

```{r warning=FALSE, message=FALSE}

new_observations <- test_data

predict(object = qda_model, newdata = new_observations)

```

Por ejemplo, según la función discriminante, la probabilidad a posteriori de que el segundo registro del conjunto test  **apruebe** es del 99.9% mientras la de que **suspenda** es tan solo del 0.1%. Por tanto este dato sería clasificado en la calificación **aprobado**. Se razona del mismo modo con el resto de elementos del conjunto test.

## Validación del modelo

La función *confusionmatrix()* del paquete *biotools* realiza una validación cruzada del modelo de clasificación.

```{r warning=FALSE, message=FALSE}

pred <- predict(object = qda_model, newdata = test_data)
confusionmatrix(test_data$G3, pred$class)

```

```{r warning=FALSE, message=FALSE}

# Classification error percentage
test_error <- mean(test_data$G3 != pred$class) * 100
paste("test_error=", test_error, "%")

```

## Visualización de las clasificaciones

La función *partimat()* del paquete *klaR* permite representar los **límites de clasificación** de un modelo discriminante lineal o cuadrático para cada par de predictores. Cada color representa una región de clasificación acorde al modelo, **se muestra el centroide** de cada región y el **valor real de las observaciones**.

```{r warning=FALSE, message=FALSE}

partimat(G3 ~ sex + age + famsize + Mjob + reason + traveltime + studytime +
              famsup + paid + activities + romantic + famrel + freetime + 
              goout + Dalc + Walc + health + absences,
         data = test_data, method = "qda", prec = 200,
         image.colors = c("darkgoldenrod1", "skyblue2"),
         col.mean = "firebrick", nplots.vert =1, nplots.hor=3)

```

**LDA vs QDA**

El clasificador más adecuado depende de las implicaciones que tenga asumir que todos los grupos comparten una matriz de covarianza común ya que este supuesto puede producir un sesgo en las clasificaciones o producir varianzas altas.

- **LDA** produce **límites de decisión lineales**, lo que se traduce en menor flexibilidad y por lo tanto menor problema de varianza.
- **QDA** produce **límites cuadráticos** y por lo tanto curvos, lo que aporta mayor flexibilidad permitiendo ajustarse mejor a los datos, menor sesgo pero mayor riesgo de varianza.
- En términos generales, **LDA tiende a conseguir mejores clasificaciones que QDA cuando hay pocas observaciones con las que entrenar al modelo**, escenario en el que evitar la varianza es crucial.
- Si se dispone de una **gran cantidad de observaciones de entrenamiento (como es nuestro caso)** o si no es asumible que existe una matriz de covarianza común entre clases, **QDA es más adecuado**.

# **Análisis cluster (AC)**

Vamos a realizar un Análisis Clúster que confirme que la agrupación de la
variable respuesta (variable *G3* categórica) utilizada en los modelos de clasificación lineal y cuadrático es adecuada.

## Algunas distancias para Análisis Cluster

Para clasificar las observaciones en grupos es necesario elegir medidas de **similaridad**, o de **distancia** (disimilaridad), adecuadas que proporcionen información de como de parecidas son dos observaciones cualesquiera. De hecho esta elección influye en el tamaño y forma de los clusters. Esta elección es un **paso fundamental** en clustering.

Algunas **medidas de distancia clásicas** frecuentemente utilizadas son la distancia **Euclídea** o la distancia **Manhattan**.

Otras medidas de disimilaridad ampliamente utilizadas, por ejemplo, en el análisis de datos de expresión génica son las **distancias basadas en correlaciones**. Estas distancias se obtienen restando la correspondiente medida de correlación al valor 1. Entre estas medidas de distancia destacan: la distancia basada en la correlación de **Pearson**, en la correlación de **Spearman**, correlación de **Kendall**, etc.

Como se ha dicho anteriormente la elección de la distancia es muy importante. Casi todo el software habitual para Análisis Cluster utiliza la distancia euclídea, aunque dependiendo del tipo de datos y de las preguntas de investigación planteadas puede interesar otra medida de disimilaridad o distancia.

En R es fácil calcular y visualizar la matriz de distancias entre observaciones con las funciones *get_dist()* y *fviz_dist()*, respectivamente, incluidas en el paquete *factoextra*. De hecho, aunque por defecto, *get_dist()* calcula la distancia euclídea también admite como parámetro todas las distancias mencionadas anteriormente.

La siguiente matriz de distancias muestra en **marrón aquellos estados que presentan grandes disimilitudes** (distancias), frente a aquellos que parecen **más cercanos en amarillo**. Se utiliza el color blanco para referirse a aquellos estados con distancias no tan extremas como para ser consideradas como bajas o altas.

```{r warning=FALSE, message=FALSE}

distance<- get_dist(data_xlsx_reduced)
fviz_dist(distance, gradient = list(low ="yellow", mid = "white", high = "brown"))

```

## **Clustering jerárquico**: método de Ward

El **agrupamiento jerárquico** está interesado en encontrar una jerarquía basada en la cercanía o semejanza de los datos según la distancia considerada. En el caso **aglomerativo**, se parte de un grupo con las observaciones más cercanas. A continuación se calculan los siguientes pares más cercanos y de manera ascendente se van generando grupos. Esta construcción se puede observar de forma visual con la construcción de un **dendrograma**.

A continuación se ilustrará cómo los grupos están definidos por la cantidad de líneas verticales del dendrograma, y la selección del número de grupos óptimo se podrá estimar desde este mismo gráfico.

```{r warning=FALSE, message=FALSE}

dendrogram <- hclust(dist(data_xlsx_reduced, method = 'euclidean'), method = 'ward.D')
ggdendrogram(dendrogram, rotate = FALSE, labels = FALSE, theme_dendro = TRUE) + 
             labs(title = "Dendrograma")

```

En el **eje horizontal** del dendrograma tenemos **cada uno de los datos** que componen el conjunto de entrada, mientras que en el **eje vertical** se representa la **distancia euclídea** que existe entre cada grupo a medida que éstos se van jerarquizando.

Cada **línea vertical** del diagrama representa un **agrupamiento**. Conforme se va subiendo en el dendograma termina con un solo gran grupo determinado por la línea horizontal superior. De modo que, **al ir descendiendo en la jerarquía**, se observa que de un solo grupo pasamos a 2, luego a 3, luego a 4, y así sucesivamente.

Una forma de determinar el número *K de grupos adecuado* es cortando el dendrograma a aquella altura del diagrama que mejor representa los datos de entrada.

## **Clustering no jerárquico**: algoritmo K-means

R implementa el **algoritmo K-means** con la función del mismo nombre. Esta función recibe como parámetros de entrada los datos y el número de agrupamientos a realizar (parámetro *centers*). Para abordar el problema de la **elección de los puntos semilla iniciales** incorpora el parámetro *nstart* que prueba múltiples configuraciones iniciales e informa sobre la mejor. Por ejemplo, si *nstart = 25*, generará 25 configuraciones iniciales. El uso de este parámetro es recomendable.

Para este primer ejemplo la función *kmeans()* construye dos clusters.

```{r warning=FALSE, message=FALSE}

k2 <- kmeans(data_xlsx_reduced, centers = 2, nstart = 25)

# Displaying all the fields of the object k2
str(k2)

```

La salida que proporciona la función *kmeans()* es una lista de información, de la que destacan las siguientes:

- *cluster*: es un vector de enteros, de 1 a K (K = 2 en este caso), que indica el cluster en el que ha sido ubicado cada observación.
- *centers*: una matriz con los sucesivos centros de los clusters.
- *totss*: la suma total de cuadrados.
- *withinss*: vector de suma de cuadrados dentro de cada cluster (un componente por cluster).
- *tot.withinss*: suma total de cuadrados de los cluster, i.e. sum(withinss).
- *betweens*: suma de cuadrados entre grupos, i.e. totss-tot.withinss.
- *size*: el número de observaciones en cada cluster.

Al mostrar la variable *k2* se ve como las agrupaciones dan como resultado 2 tamaños de agrupación de 44 y 351 También se ven los centros de cada grupo (medias) en todas las variables. Y por último la asignación de grupo para cada observación.

```{r warning=FALSE, message=FALSE}

k2

```

Una forma visual de resumir los resultados de forma elegante y con una interpretación directa es mediante el uso de la función *fviz_cluster()*.

```{r warning=FALSE, message=FALSE}

fviz_cluster(k2, data=data_xlsx_reduced)

```

**Observación**: Si hay más de dos dimensiones (variables), esta función realizará, en primer lugar, un análisis de componentes principales (ACP) y dibujará los puntos de acuerdo a las dos primeras componentes principales obtenidas (las que explican la mayor parte de la varianza). Es por esto que en el gráfico anterior, Dim1 y Dim2 se refieren a estas dos componentes principales.

Para usar el algoritmo K-means, el número **K de clusters debe ser fijado de antemano**. Es por esto que es recomendable ejecutar el mismo proceso con otros valores de K (en este ejemplo para K = 3, 4 y 5) para comparar y examinar las diferencias entre los resultados.

```{r warning=FALSE, message=FALSE}

set.seed(123)

k3 <- kmeans(data_xlsx_reduced, centers = 3, nstart = 25)
k4 <- kmeans(data_xlsx_reduced, centers = 4, nstart = 25)
k5 <- kmeans(data_xlsx_reduced, centers = 5, nstart = 25)

# Plots to compare
p24 <- fviz_cluster(k2, geom = "point", data = data_xlsx_reduced) + ggtitle("k = 2")
p25 <- fviz_cluster(k3, geom = "point",  data = data_xlsx_reduced) + ggtitle("k = 3")
p26 <- fviz_cluster(k4, geom = "point",  data = data_xlsx_reduced) + ggtitle("k = 4")
p27 <- fviz_cluster(k5, geom = "point",  data = data_xlsx_reduced) + ggtitle("k = 5")


grid.arrange(p24, p25, p26, p27, nrow = 2)

```

Aunque esta visualización nos permite deducir donde ocurren las verdaderas diferencias (o no ocurren, ya que vemos que los clusters se superponen), **no nos dice cuál es el número óptimo de clusters**.

### Determinación del número óptimo de clusters

Tal y como se ha indicado anteriormente, cuando se aplica un método no jerárquico como K-means para realizar un análisis cluster, el investigador debe informar a priori del número de clusters deseado. En este sentido, este investigador estará interesado en **proporcionar de partida un número óptimo de grupos a formar**.

A continuación se presentan tres de los métodos más utilizados para determinar este número óptimo de grupos: método de Elbow, método de Silhouette y el stadístico Gap.

#### Método de Elbow

Teniendo en mente que la idea tras una división en K clusters, es obtener estas agrupaciones de modo que la varianza total intra-grupos sea mínima (total within-cluster variation o total within-cluster sum of square), se puede utilizar el siguiente algoritmo para identificar el número óptimo de clusters:

- Ejecutar un algoritmo de clustering (como K-means) para diferentes valores del K (por ejemplo K = 1,…,10).
- Para cada K se calcula la variación total intra-cluster (total within-cluster sum of square, que aquí denotamos por *wss*).
- Se dibuja la curva de *wss* de acuerdo al número de clusters K.
La localización en esta curva de un "codo" es tomado como el indicador más apropiado del número de clusters.

Aunque podemos programar este algoritmo en R, el método de Elbow está implementado en la función *fviz_nbclust()*.

```{r warning=FALSE, message=FALSE}

set.seed(123)
fviz_nbclust(data_xlsx_reduced, kmeans, method = "wss")

```

#### Método de Silhouette

Este enfoque, *silueta promedio*, mide la calidad de un cluster. Es decir, determina como de adecuado es un objeto dentro de su cluster. El número óptimo de clusters según este enfoque es, de entre un rango de valores posibles para K, aquel que maximiza la silueta promedio.

Como antes, este algoritmo se puede programar en R, pero la función *fviz_nbclust()* también lo incluye.

```{r warning=FALSE, message=FALSE}

set.seed(123)
fviz_nbclust(data_xlsx_reduced, kmeans, method = "silhouette")

```

#### Método estadístico de brecha (GAP)

Este método compara la variación total intracluster para diferentes valores de K. Utiliza simulación Montecarlo en su algoritmo.

La función *clusGap()* proporciona el estadístico GAP y su error estándar para una salida. Con la función *fviz_gap_stat()* se obtiene una representación gráfica que sugiere un número de clusters.

```{r warning=FALSE, message=FALSE}

set.seed(123)
gap_stat <- clusGap(data_xlsx_reduced, FUN = kmeans, nstart = 25, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)

```

### Análisis de los resultados

Tras el análisis pormenorizado del número óptimo de clusters, realizado en la sección anterior, y las pruebas anteriores con diferentes números de clusters, concluímos que **no parece adecuado realizar un Análisis Cluster**. Esto puede deberse quizá a la independencia de las variables que observamos a la hora de realizar ACP o AF o la forma en la que se han codificado las variables categóricas (podría probarse a hacer este análisis cluster usando medidas de distancia para variables categóricas, como puede ser la distancia de Manhattan). Se observa que a mayor número de clusters, peor (ya que los clusters se superponen)

Aun así, **si tuviesemos que realizarlo, K = 2 parece ser el número de grupos más adecuado**. Esto confirma que **la agrupación de la variable respuesta utilizada en los modelos de clasificación lineal y cuadrático es adecuada**. Por lo tanto, el categorizar la calificación de los alumnos como *aprobado* o *suspenso*, fue una buena opción tomada a la hora de realizar los modelos de clasificación (por ejemplo, podríamos haber pensado también en categorizar la calificación de los alumnos como *suspenso*, *aprobado*, *notable* y *sobresaliente*).

```{r warning=FALSE, message=FALSE}

set.seed(123)
final <- kmeans(data_xlsx_reduced, centers = 2, nstart = 25)
print(final)

```

```{r warning=FALSE, message=FALSE}

# Final output
fviz_cluster(final, data = data_xlsx_reduced)

```
